# Reinforcement Learning on Highway-env

This project focuses on implementing and experimenting with Reinforcement Learning (RL) methods on the Highway-env collection of environments. The project is divided into three main parts, each focusing on a different aspect of RL.

## Table of Contents
1. [Preliminary](#preliminary)
2. [Highway Fast Environment](#highway-fast-environment)
3. [Continuous Actions](#continuous-actions)
4. [Reference Implementations: Stable Baselines](#reference-implementations-stable-baselines)
5. [Results and Discussion](#results-and-discussion)

## Preliminary <a name="preliminary"></a>
Before starting the project, it is essential to understand the environment's documentation and configuration. The Highway-env allows for multiple configurations of observations and actions, leading to significant changes in the environment. The actions can be either discrete or continuous, and the set of available actions may change depending on the state.

## Highway Fast Environment <a name="highway-fast-environment"></a>

In the Highway Fast Environment task, the goal is to solve the `highway-fast` environment using a Deep Q-Network (DQN) implemented from scratch. The code for this task is located in the Jupyter notebook `highway_fast_dqn.ipynb`.

The notebook covers the implementation of a DQN approach to solve the `highway-fast` environment. The environment's configuration is fixed according to the project instructions provided in `config.py`.

## Continuous Actions <a name="continuous-actions"></a>


## Reference Implementations: Stable Baselines <a name="reference-implementations-stable-baselines"></a>


## Results and Discussion <a name="results-and-discussion"></a>
This section presents the results of the experiments conducted in the three tasks and discusses the findings. The performance of the agents, the differences between discrete and continuous actions, and the insights from the experiment using Stable Baselines are covered.


